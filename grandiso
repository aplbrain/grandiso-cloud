#!/usr/bin/env python3

# Type imports:
from typing import Optional, Tuple

# Standard library imports:
import argparse
import glob
import io
import json
import logging
import os
import sys
import time
import zipfile
from zipfile import ZipFile, ZipInfo

# Installed imports:
import boto3
import networkx as nx
import pandas as pd
import tqdm

# Note that for certain commands, you will also need to install Grand/GrandIso.

DEBUG = True

logging.basicConfig(level=logging.INFO)


queue_name_base = "GrandIsoQ"
function_name_base = "GrandIsoLambda"

ASSUME_POLICY = """{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": [
          "apigateway.amazonaws.com",
          "lambda.amazonaws.com",
          "events.amazonaws.com"
          "sqs.amazonaws.com"
        ]
      },
      "Action": "sts:AssumeRole"
    }
  ]
}"""


class PermissiveZipFile(ZipFile):
    """
    A Zipfile class that also sets file permission flags.

    This is required for the lambda runtime. Thanks to the answerers at
    https://stackoverflow.com/questions/434641/ for insights and useful code.

    """

    def writestr(self, zinfo_or_arcname, data, compress_type=None):
        if not isinstance(zinfo_or_arcname, ZipInfo):
            zinfo = ZipInfo(
                filename=zinfo_or_arcname, date_time=time.localtime(time.time())[:6]
            )

            zinfo.compress_type = self.compression
            if zinfo.filename[-1] == "/":
                zinfo.external_attr = 0o40775 << 16  # drwxrwxr-x
                zinfo.external_attr |= 0x10  # MS-DOS directory flag
            else:
                zinfo.external_attr = 0o664 << 16  # ?rw-rw-r--
        else:
            zinfo = zinfo_or_arcname

        super(PermissiveZipFile, self).writestr(zinfo, data, compress_type)


def _create_dynamo_table(
    table_name: str, primary_key: str, client, read_write_units: Optional[int] = None,
):
    """
    Create a new dynamoDB table.

    """
    if read_write_units is not None:
        raise NotImplementedError("Non-on-demand billing is not currently supported.")

    return client.create_table(
        TableName=table_name,
        KeySchema=[
            # Partition/hash key
            {"AttributeName": primary_key, "KeyType": "HASH"},
        ],
        AttributeDefinitions=[{"AttributeName": primary_key, "AttributeType": "S"},],
        # Currently do not support billing methods besides on-demand.
        BillingMode="PAY_PER_REQUEST",
    )


def _dynamo_table_exists(table_name: str, client: boto3.client):
    """
    Check to see if the DynamoDB table already exists.

    Returns:
        bool: Whether table exists

    """
    existing_tables = client.list_tables()["TableNames"]
    return table_name in existing_tables


class GrandIso:
    """
    A class responsible for creating, managing, and manipulating GrandIso jobs
    in the cloud.

    This system currently only supports running locally, and running one job
    at a time before it is torn down and restarted.

    """

    def __init__(self, dry: bool = False):
        """
        Create a new GrandIso client.

        Arguments:
            dry (bool: False): Whether to try-run (True) or actually perform
                changes (False). Defaults to False (i.e. will actually kick
                off a job when you ask it to.)

        """
        self.dry = dry
        self.lambda_client = boto3.client(
            "lambda",
            endpoint_url="http://localhost:4566",
            aws_access_key_id="foo",
            aws_secret_access_key="foo",
        )
        self.iam_client = boto3.client(
            "iam",
            endpoint_url="http://localhost:4566",
            aws_access_key_id="foo",
            aws_secret_access_key="foo",
        )
        self.sqs_client = boto3.client(
            "sqs",
            endpoint_url="http://localhost:4566",
            aws_access_key_id="foo",
            aws_secret_access_key="foo",
        )
        self.dynamo_client = boto3.client(
            "dynamodb",
            endpoint_url="http://localhost:4566",
            aws_access_key_id="foo",
            aws_secret_access_key="foo",
        )

        self.results_table_name = "GrandIsoResults"
        self.endpoint_url = "http://localhost:4566"
        self._lambda_execution_role_name = "GrandIsoLambdaExecutionRole"

        self.log = logging

    def cli_teardown(self, argparser_args=None):
        self.log.debug("Starting the teardown process.")
        if self.dry:
            self.log.info("This will tear down the following resources:")
            self.log.info(
                f" - (IAM) Lambda Execution Role: {self._lambda_execution_role_name}"
            )
            self.log.info(f" - (Lambda) Lambda Function:    {function_name_base}")
            self.log.info(f" - (SQS) Queue:                 {queue_name_base}")
            self.log.info(f" - (DynamoDB) Table:            {self.results_table_name}")
            return
        self.log.debug("Completed teardown process.")

    def create_host(self):
        import grand
        from grand.backends import DynamoDBBackend

        G = grand.Graph(backend=DynamoDBBackend(dynamodb_url=self.endpoint_url))

        G.nx.add_edge("A", "B")
        G.nx.add_edge("B", "C")
        G.nx.add_edge("C", "A")

    def _scan_table(self, table, scan_kwargs: dict = None):
        done = False
        start_key = None
        results = []
        scan_kwargs = scan_kwargs or {}
        while not done:
            if start_key:
                scan_kwargs["ExclusiveStartKey"] = start_key
            response = table.scan(**scan_kwargs)
            results += response.get("Items", [])
            start_key = response.get("LastEvaluatedKey", None)
            done = start_key is None
        return results

    def aggregate_results(self):
        dynamodb_resource = boto3.resource(
            "dynamodb",
            endpoint_url=self.endpoint_url,
            aws_access_key_id="foo",
            aws_secret_access_key="foo",
        )
        results_table = dynamodb_resource.Table(self.results_table_name)
        return self._scan_table(results_table)

    def cli_results(self, argparser_args=None):
        if argparser_args.format == "csv":
            return pd.DataFrame(
                [res["candidate"] for res in self.aggregate_results()]
            ).to_csv()
        if argparser_args.format == "json":
            return pd.DataFrame(
                [res["candidate"] for res in self.aggregate_results()]
            ).to_json()
        if argparser_args.format == "raw":
            return self.aggregate_results()

    def print_cli_results(self, argparser_args=None):
        print(self.cli_results(argparser_args))

    # def reset(self):
    #     # The grand iso is dead!
    #     self.purge_queue()
    #     self.teardown_lambda()

    #     # Long live the grand iso!
    #     self.provision()

    def provision(self) -> Tuple[str, str, str]:
        queue_arn = self.create_queue()
        lambda_arn = self.create_lambda()
        table_arn = self.create_table()
        return (queue_arn, lambda_arn, table_arn)

    def cli_provision(self, argparser_args=None):
        self.log.debug("Starting the resource provisioning process.")
        if self.dry:
            self.log.info("This will provision the following resources:")
            self.log.info(
                f" - (IAM) Lambda Execution Role: {self._lambda_execution_role_name}"
            )
            self.log.info(f" - (Lambda) Lambda Function:    {function_name_base}")
            self.log.info(f" - (SQS) Queue:                 {queue_name_base}")
            self.log.info(f" - (DynamoDB) Table:            {self.results_table_name}")
            return

        (queue_arn, lambda_arn, table_arn) = self.provision()

        self.log.debug(f"Created table with ARN [{table_arn}].")

        self.log.debug(f"Attaching lambda [{lambda_arn}] to queue [{queue_arn}].")
        self.attach_queue_event(queue_arn, lambda_arn)
        self.log.debug("Completed resource provisioning process.")

    def cli_kickoff(self, argparser_args=None):
        if self.dry:
            self.log.info("This will begin the motif search in the graph.")
            return

        motif = nx.DiGraph()
        motif.add_edge("A", "B")
        motif.add_edge("B", "C")
        motif.add_edge("C", "A")
        # TODO: construct the motif from disk:

        self.kickoff(motif)

    def kickoff(self, motif_nx: nx.Graph, initial_candidate: dict = None):

        initial_queue_item = json.dumps(
            {
                # A version of this motif:
                "motif": nx.readwrite.node_link_data(motif_nx),
                # An empty candidate (formerly called "backbone"):
                "candidate": initial_candidate or {},
                # An arbitrary ID as PK:
                # TODO: uuid4
                "ID": 1,
            }
        )
        # TODO: Do something cleverer than printing this:
        self.log.debug(self.queue_push(initial_queue_item))

    def purge_queue(self):
        queue_url = self.sqs_client.get_queue_url(QueueName=queue_name_base)["QueueUrl"]
        self.sqs_client.purge_queue(QueueUrl=queue_url)

    def teardown_lambda(self):
        self.lambda_client.delete_function(FunctionName=function_name_base)

    def attach_queue_event(self, queue_arn, lambda_arn):
        self.lambda_client.create_event_source_mapping(
            EventSourceArn=queue_arn,
            FunctionName=lambda_arn,
            Enabled=True,
            BatchSize=1,
        )

    def queue_push(self, value):
        queue_url = self.sqs_client.get_queue_url(QueueName=queue_name_base)["QueueUrl"]
        self.sqs_client.send_message(QueueUrl=queue_url, MessageBody=value)

    def create_queue(self):
        new_queue_request = self.sqs_client.create_queue(
            QueueName=queue_name_base, Attributes={"FifoQueue": "false"}
        )

        queue_arn = self.sqs_client.get_queue_attributes(
            QueueUrl=new_queue_request["QueueUrl"], AttributeNames=["QueueArn"]
        )["Attributes"]["QueueArn"]
        self.queue_arn = queue_arn

        return queue_arn

    def generate_zip(self):
        """
        Construct a zip file from the vendored and nonvendored libraries.

        """

        def zipdir(dpath, zipf):
            """
            Zip an entire directory, recursively.
            """
            _debug_wrap = tqdm.tqdm if DEBUG else lambda x: x
            for fp in _debug_wrap(
                glob.glob(os.path.join(dpath, "**/*"), recursive=True)
            ):
                base = os.path.commonpath([dpath, fp])
                zipf.write(fp, arcname=fp.replace(base + "/", ""))

        mem_zip = io.BytesIO()

        # A list of vendor directories:
        vendor_dirs = ["lambda/vendor"]

        # A list of files to include at root level:
        files = ["lambda/main.py"]

        with PermissiveZipFile(
            mem_zip, mode="w", compression=zipfile.ZIP_DEFLATED
        ) as zf:
            for directory in vendor_dirs:
                zipdir(directory, zf)

            for file in files:
                zf.write(file, arcname=file.replace("lambda/", ""))

        return mem_zip.getvalue()

    def create_lambda(self):
        """
        Create a lambda for GrandIso in general.

        Note that this is not specific to a job, and so it is not prefixed with
        namespacing variables.

        """
        # First, create a lambda execution role in IAM.
        # This means creating a Policy for a user:
        try:
            role = self.iam_client.get_role(RoleName=self._lambda_execution_role_name)
        except:
            role = self.iam_client.create_role(
                RoleName=self._lambda_execution_role_name,
                AssumeRolePolicyDocument=ASSUME_POLICY,
            )
        role_arn = role["Role"]["Arn"]

        # Next, create the lambda itself:
        try:
            lambda_function = self.lambda_client.get_function(
                FunctionName=function_name_base
            )
            lambda_arn = lambda_function["Configuration"]["FunctionArn"]
        except:
            lambda_function = self.lambda_client.create_function(
                FunctionName=function_name_base,
                Runtime="python3.8",
                Role=role_arn,
                Handler="main.main",
                Code={"ZipFile": self.generate_zip()},
            )
            # print(lambda_function)
            lambda_arn = lambda_function["FunctionArn"]
        return lambda_arn

    def create_table(self):
        """
        Create a DynamoDB table to hold the results from this job.

        """
        if not _dynamo_table_exists(self.results_table_name, self.dynamo_client):
            _create_dynamo_table(self.results_table_name, "ID", self.dynamo_client)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="GrandIso subgraph isomorphism in the cloud"
    )

    # Global arguments:

    parser.add_argument(
        "--job-name",
        type=str,
        required=True,
        help=(
            "The job name to use to refer to this subgraph isomorphism "
            + "search. Make one up when calling `provision`, and then reuse "
            + "the same name for the other commands."
        ),
    )
    parser.add_argument(
        "--dry",
        type=bool,
        required=False,
        default=False,
        help="Whether to dry-run (instead of actually running)",
    )

    grandiso = GrandIso(dry=True)

    subparsers = parser.add_subparsers(help="Sub-commands.")

    # Provision
    provision_command = subparsers.add_parser(
        "provision", help="Provision resources for GrandIso."
    )
    provision_command.set_defaults(func=grandiso.cli_provision)

    # Teardown
    teardown_command = subparsers.add_parser(
        "teardown", help="Remove all traces of GrandIso from this AWS account."
    )
    teardown_command.set_defaults(func=grandiso.cli_teardown)

    # Kickoff
    kickoff_command = subparsers.add_parser(
        "kickoff", help="Start the search for a given motif."
    )
    kickoff_command.add_argument(
        "--motif",
        type=str,
        required=True,
        help=(
            "The motif for which to begin searching. Should be a file that "
            + "contains a .motif definition in the DotMotif DSL. Note that "
            + "only structural (edges and nodes) and attribute information "
            + "(a.weight == 5) will be searched; attribute comparator "
            + "information (a.weight > 5) will be omitted.",
        ),
    )
    kickoff_command.set_defaults(func=grandiso.cli_kickoff)

    # Kickoff
    results_command = subparsers.add_parser(
        "results", help="Print the results from the search."
    )
    results_command.add_argument(
        "--format",
        default="csv",
        type=str,
        required=False,
        help="The format (csv|json) in which to return results.",
    )
    results_command.set_defaults(func=grandiso.print_cli_results)

    # Currently does not support `reset`. I think that's correct...

    # Parse args:
    args = parser.parse_args()
    grandiso.dry = args.dry
    args.func(args)

    # if sys.argv[-1] == "create_host":
    #     GrandIso().create_host()
    # if sys.argv[-1] == "aggregate_results":
    #     for res in GrandIso().aggregate_results():
    #         print(res)
